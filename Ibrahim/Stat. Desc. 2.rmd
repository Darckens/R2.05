---
title: "Statistiques Descriptives 2"
author: "Ibrahim BENKHERFELLAH & Axel COULET"
date: "2025-02-21"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
lang: fr
---

```{r eval=FALSE, include=FALSE}
# Exécutez ceci la première fois que vous ouvrez le document pour installer tous les packages requis

list.of.packages <- c("ggplot2", "ggthemes", "extrafont", "gridExtra", "lmtest")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[, "Package"])]
if (length(new.packages)) install.packages(new.packages)
```

```{r echo=FALSE, message=FALSE}
# Exécutez ces lignes au début

library(ggplot2)
library(ggthemes)
library(extrafont)
library(gridExtra)
library(lmtest)
```

```{r eval=FALSE, include=FALSE}
# FR : Exécutez ceci une seule fois

font_import()
loadfonts()
```


# __Exercice n°1__

## __1.__\

<br>
Tout d'abord, qu'est-ce qu'une variable explicative et une variable expliquée ?

-   Une variable explicative est la variable qui permet d'expliquer une autre variable. Elle est souvent notée $X$. Sur une représentation graphique, la variable explicative est représentée en abscisse.

-   Une variable expliquée est la variable pour laquelle on cherche un modèle de prévision. Son évolution dépend d'une ou de plusieurs autres variables. Elle est souvent notée $Y$. Sur une représentation graphique, la variable expliquée est représentée en ordonnée.\

Ainsi, dans le cadre de notre exercice, la variable explicative est le **dosage de sérum X (en mL)**. Et la variable expliquée est la **quantité de bactéries Y (en UFC/mL)**, car c'est celle qui est mesurée en fonction du dosage de sérum et qui dépend potentiellement de $X$.\
<br>

## __2.__\

Représentation du nuage de points de la statistique bi-variée ($x_{i};y_{i}$) :

<br>
```{r echo=FALSE, fig.align='center'}

# Lecture des données
data <- read.csv("../donnees_r.csv", header = TRUE,  sep = ",", colClasses = c("numeric", "numeric"))

# Nuage de points
ggplot(data, aes(x = X, y = Y)) +
  geom_point(color = "red", size = 2) +
  labs(
    title = "Nuage de points : Dosage vs Bactéries",
    x = "Dosage de sérum (mL)",
    y = "Quantité de bactéries (UFC/mL)"
  ) +
  theme_fivethirtyeight() +
  theme(axis.title = element_text(), text = element_text(family = "Segoe UI", size = 14))

```
<br>

## __3.__\

### a\) Calcul de la moyenne et de la variance de la quantité de bactéries

<br>

```{r echo=FALSE }

# Calcul de la moyenne et de la variance (de Pearson) de la quantité de bactéries
var_Y_pearson <- sum((data$Y - mean(data$Y))^2) / length(data$Y)

cat("Moyenne de la quantité de bactéries : ", mean(data$Y), "\n")
cat("Variance de la quantité de bactéries : ", var_Y_pearson)

```

---

- Moyenne : __$\overline{y} = 0,4389205$__

- Variance : __$\sigma^2 = 0,0671159$__

<br>

::: {.alert .alert-warning}
**Attention :** Ici, la variance est calculée selon la formule de König-Huygens. Il s'agit de la moyenne des carrés des écarts à la moyenne.
:::

<br>

### b\) Box-plot de la dsitribution de la variable : __Quantité de bactéries__

<br>
```{r echo=FALSE, fig.align='center'}

# Boîte à moustache de la distribution de la quantité de bactéries
ggplot(data, aes(y = Y)) +
  geom_boxplot() +
  labs(title = "Boîte à moustache de la distribution\nde la quantité de bactéries") +
  theme_fivethirtyeight() +
  theme(text = element_text(family = "Segoe UI", size = 14))
```

<br>

### c\) Analyse des résultats obtenus

L'analyse de la quantité de bactéries montre une moyenne de __$0,4389205$__ et une variance de __$0,0671159$__, indiquant une dispersion modérée. Le diagramme en boîte révèle une distribution symétrique avec une médiane autour de __$0,4$__ et un intervalle interquartile de __$0,23$__ à __$0,60$__, suggérant une répartition homogène des données.

## __4.__

Nous savons que la Covariance de $X$ et $Y$ est : $$Cov(X,Y) = \frac{1}{N}\sum_{i=1}^{i=N}(X_i - \overline{X})(Y_i - \overline{Y}) = (\frac{1}{N}\sum_{i=1}^{i=N}(X_iY_i)) - \overline{X}\overline{Y}$$

---

```{r echo=FALSE }

# Calcul de la covariance entre X et Y 

covariance <- sum(data$X * data$Y) / length(data$X) - mean(data$X) * mean(data$Y)


cat("Covariance entre X et Y : ", covariance)

```

---

On trouve donc : __$Cov(X, Y) = -0,7243548$__\

La covariance entre les deux variables étant de __$-0,7243548$__, cela indique une assez forte relation. Le signe étant négative, cela  indique que $X$ et $Y$ évoluent de manière opposé. Autrement dit, lorsque $X$ augmente, $Y$ quant à lui à diminuer, et inversement. Ce qui coincide avec l'observation faite ci-dessus, dans le nuage de point.

## __5.__

Les Q-Q plots (Quantile-Quantile plots) permettent de comparer les quantiles d'une variable observée à ceux d'une distribution théorique, souvent la distribution normale. En examinant l'alignement des points sur une droite diagonale, il est possible d'évaluer visuellement si une variable suit une distribution normale.

### a\) Etude graphique Quantile-Quantile

<br>
```{r echo=FALSE, fig.align='center'}

# Diagramme Quantile-Quantile (Q-Q plots) pour X
qqX <- ggplot(data) +
  geom_qq(aes(sample = X), geom = "point", position = "identity") +
  ggtitle("Q-Q Plot de X") +
  theme_fivethirtyeight() +
  theme(text = element_text(family = "Segoe UI", size = 14))

# Diagramme Quantile-Quantile (Q-Q plots) pour Y
qqY <- ggplot(data) +
  geom_qq(aes(sample = Y), geom = "point", position = "identity") +
  ggtitle("Q-Q Plot de Y") +
  theme_fivethirtyeight() +
  theme(text = element_text(family = "Segoe UI", size = 14))

# Affichage des Q-Q plots
grid.arrange(qqX, qqY, ncol = 2)
```
<br>

#### Interprétation du Q-Q Plot pour la variable `X`

- **Alignement des points** :  
  Les points du Q-Q plot de **$X$** sont quasiment alignés sur la droite diagonale. Cela suggère que les quantiles observés de **$X$** correspondent étroitement aux quantiles théoriques d'une distribution normale.

- **Extrémités (queues de distribution)** :  
  Les points situés aux extrémités ne s'écartent pas significativement de la droite, ce qui renforce l'idée que **$X$** présente les caractéristiques d'une distribution normale.

La variable **$X$ semble être normalement distribuée.**

---

#### Interprétation du Q-Q Plot pour la variable `Y`

- **Écart par rapport à la diagonale** :  
  Pour la variable **$Y$**, les points du Q-Q plot ne suivent pas la droite diagonale. On observe une courbure, surtout aux extrémités, indiquant que les quantiles observés ne correspondent pas aux quantiles attendus pour une distribution normale.

- **Queues de distribution** :  
  Les écarts importants aux extrémités suggèrent que **$Y$** possède des caractéristiques non conformes à la normalité, telles qu'une asymétrie ou des queues de distribution différentes de celles d'une loi normale.

La variable **$Y$ ne semble pas suivre une distribution normale**.


### b\) Etude numérique Shapiro-Wilk

<br>
Le **test de Shapiro-Wilk** est un test statistique permettant d’évaluer si un échantillon suit une **distribution normale**. Il est particulièrement efficace pour les **petites tailles d’échantillon** ($n \leq 50$), bien qu'il puisse être utilisé sur des ensembles de données plus grands.

---

#### Hypothèses du test

<br>

- **$H_0$ (Hypothèse nulle) :** Les données suivent une distribution normale.

---

#### Formule du test

La statistique du test de Shapiro-Wilk est définie comme suit :

$$W = \frac{( \sum_{i=1}^{n} a_i x_{(i)} )^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} $$

**où** :

- $x_{(i)}$ sont les valeurs de l’échantillon classées par ordre croissant.
- $\bar{x}$ est la **moyenne** de l’échantillon.
- $a_i$ sont des coefficients calculés à partir des quantiles d'une distribution normale.

---

#### Interprétation des résultats

- Si la **p-valeur** du test est **supérieure à 0.05**, on **ne rejette pas** $H_0$ : l’échantillon peut être considéré comme **normalement distribué**.
- Si la **p-valeur** est **inférieure à 0.05**, on **rejette** $H_0$ : les données **ne suivent pas** une distribution normale.

---

#### Résultats du code R

```{r echo=FALSE}

# Test de Shapiro-Wilk pour les variables X et Y
shapiro.test(data$X)
shapiro.test(data$Y)
```

---

#### Variable `X`  

Le test appliqué à **`X`** donne une statistique **W = 0.95558** avec une **p-valeur de 0.05809**.

Puisque la **p-valeur est supérieure à 0.05**, nous **ne rejetons pas** l'hypothèse de normalité.

Cela signifie que **la variable `X` suit une distribution normale** au seuil de 5%.  

---

#### Variable `Y`  

Le test appliqué à **`Y`** donne une statistique **W = 0.92148** avec une **p-valeur de 0.002668**.  

Dans ce cas, la **p-valeur est inférieure à 0.05**, nous **rejetons donc** l'hypothèse de normalité. 

Ainsi, nous pouvons conclure que **la variable `Y` ne suit pas une distribution normale**.  


## __6.__

On sait que le coefficient de corrélation linéaire de Pearson se calcul de la façon suivante :

$$ r(X,Y) = Corr(X,Y) = \frac{Cov(X,Y)}{\sigma_X^2\sigma_Y^2}$$
Avec R, on trouve : 

```{r echo=FALSE}

# Calcul du coefficient de corrélation linéaire de Pearson
cor.test(data$X, data$Y, method = "pearson")
```

---

Le coefficient de corrélation linéaire de Pearson est de __$-0,9493844$__. Ce coefficient est très proche de -1, ce qui indique une forte corrélation négative entre les deux variables. Cela signifie que $X$ et $Y$ évoluent de manière opposée (comme supposé avec la covariance). Cette relation est donc confirmée par le coefficient de corrélation élevé, qui suggère une forte dépendance linéaire entre les deux variables. 
<br>

## __7.__

# Introduction
On sait qu'une droite affine s'exprime de la façon suivante : **$y = ax + b$**

Par la méthode de Legendre, aussi appelée méthode des moindres carrés, on peut trouver les coefficients $a$ et $b$ de la droite de régression linéaire qui minimise la somme des carrés des résidus. Autrement dit, on cherche $\hat{a}$ et $\hat{b}$ tels que :

$$f(\hat{a},\hat{b}) = \arg\min_{(a,b)}\sum_{i=1}^N(y_i - (ax + b))^2$$ 

Cela signifie que nous cherchons les valeurs spécifiques de $a$ et $b$ qui rendent la somme des carrés des erreurs aussi petite que possible.

Dans un modèle linéaire, le meilleur coefficient directeur (au sens des moindres carrés) et la meilleure ordonnée à l'origine (au sens des moindres carrés) sont :

$$
\hat{a} = \frac{\text{Cov}(X, Y)}{\sigma^2}
$$
**et**

$$
\hat{b} = \bar{y} - \hat{a}x 
$$

# Démonstration

$$
\frac{\partial f}{\partial a} = \sum_{i=1}^N (2*(y_i - ax_i -b)(-x_i))
$$

$$
\frac{\partial f}{\partial b} = \sum_{i=1}^N (2*(y_i - ax_i -b)(-1))
$$

**On cherche :** $~~ \frac{\partial f}{\partial a} = 0~~$ et  $~~ \frac{\partial f}{\partial b} = 0~~$

D’où le système suivant :

$$
\begin{cases}
\sum 2(y_i - ax_i - b)(-x_i) = 0\\
\sum 2(y_i - ax_i - b)(-1) = 0
\end{cases}
$$

En divisant par 2 :

$$
\begin{cases}
\sum (y_i - ax_i - b)(-x_i) = 0\\
\sum (y_i - ax_i - b)(-1) = 0
\end{cases}
$$

En divisant par $N$ :

$$
\begin{cases}
\overline{(y - ax - b)x} = 0\\
\overline{y - ax - b} = 0
\end{cases}
$$

Ce qui donne :

$$
\begin{cases}
\overline{xy} - a\overline{x^2} - b\bar{x} = 0\\
\bar{y} - a\bar{x} - b = 0
\end{cases}
$$

Multiplication de la seconde équation par $\bar{x}$ :

$$
\begin{cases}
\overline{xy} - a\overline{x^2} - b\bar{x} = 0\\
\bar{x} \bar{y} - a\overline{x}^2 - b\bar{x} = 0
\end{cases}
$$

Soustraction des deux équations :

$$
(\overline{xy} - \bar{x} \bar{y}) - a(\overline{x^2} - \overline{x}^2) = 0
$$

Ce qui donne :

$$
\text{Cov}(X,Y) - a\sigma_X^2 = 0
$$

D'où :

$$
\hat{a} = \frac{\text{Cov}(X, Y)}{\sigma_X^2} ~~~ \text{et} ~~~ \hat{b} = \bar{y} - \hat{a}\bar{x}
$$


---

### Applications 

```{r}

# Calcul de la variance de X

var_X_pearson <- sum((data$X - mean(data$X))^2) / length(data$X)

# Calcul des coefficients a et b
a <- covariance / var_X_pearson
b <- mean(data$Y) - a * mean(data$X)

cat("a =", a, "b =", b)
```


<br> 

$$
\hat{a} = \frac{-0,7243548}{8,673469} = -0.08351385
$$

**et**

$$
\hat{b} = 0,4389205 - (−0.08351385*5) = 0.8564897
$$ 

--- 

### Récapitulatif

<br>
$$\begin{cases}
\hat{a} = 0,083513\\
\hat{b} = 0,856489
\end{cases}
~ ~ \Longrightarrow  ~ ~ ~ \hat{y} ~ = ~ -0,083513x ~ + ~ 0,856489$$

<br>
```{r echo=FALSE, fig.align='center'}

# Nuage de points avec droite de régression
ggplot(data, aes(x = X, y = Y)) +
  geom_point(color = "red", size = 2) +
  geom_abline(slope = -0.083513, intercept = 0.856489, color = "blue", linewidth = 1) +
  labs(
    title = "Nuage de points : Dosage vs Bactéries",
    x = "Dosage de sérum (mL)",
    y = "Quantité de bactéries (UFC/mL)"
  ) +
  theme_fivethirtyeight() +
  theme(axis.title = element_text(), text = element_text(family = "Segoe UI", size = 14))
```
<br>

## __8.__

En observant le nuage de points, on voit clairement que lorsque le dosage de sérum augmente, la quantité de bactéries diminue de manière régulière. La droite de régression, qui semble plutôt passer au milieu de ces données, reflète bien cette tendance. Cette modélisation linéaire nous paraît être une première approximation logique et cohérente, en accord avec l'idée que l'augmentation du dosage se traduit par une diminution de la concentration bactérienne. L'ajustement linéaire offre ici une représentation intuitive et assez satisfaisante de la relation observée.

## __9.__

```{r echo=FALSE}

# Ajustement du modèle linéaire
LMS <- lm(data$Y ~ data$X)

# Résumé du modèle linéaire
summary(LMS)

#plot(summary(LMS)$residuals, main = "Distribution des résidus", ylab = "Résidus", xlab = "Observations")
#abline(h = 0, col = "red", lwd = 2)

```

---

### Test de significativité globale du modèle

Avant d'interpréter les résultats, il convient d'évaluer la significativité du modèle. Il désigne le seuil à partir duquel les résultats d'un test sont jugés fiables. Autrement dit, ce seuil détermine la confiance dans la corrélation entre un test effectué et les résultats obtenus.

Soit : $H_0$ -> le modèle n'est pas significatif 

Ce test est basé sur la **statistique de Fisher** présentée en bas de la sortie R. La p-valeur associée à cette statistique est de **`2e-16`**. Puisque cette valeur est **inférieure à 1%**, nous **rejetons l'hypothèse nulle**, à savoir le modèle est bien **globalement significatif**

---

### Interprétation des coefficients

Les coefficients permettent de comprendre comment la variable explicative $X$ influence la variable dépendante $Y$.

**`(Intercept)`** qui est dans notre cas $~\hat{b}~$ (que nous allons appelé $~\beta_0$), est de **0.85649**. Et **`data$X`** qui est $~\hat{a}~$ (que nous allons appelé $~\beta_1$), est de **-0.08351**. Ces valeurs correspondent à celles que nous avons calculées précédemment.

Les **`Pr(>|t|)`** sont les **`p-valeurs`** associées à chaque coefficient. Elles permettent de tester **l'hypothèse nulle** $H_0$ suivante : 

- $H_0 : \beta_i = 0 ~$ -> Cela signifie que $X$ n'a aucun effet sur $Y$

Or, ici les **p-valeurs sont très faibles** (**`2e-16`**), ce qui signifie que nous **rejetons l'hypothèse nulle**. Donc **les coefficients sont significatifs** et que la variable explicative $X$ a un effet significatif sur la variable dépendante $Y$. 

De plus, dans R la signification des coefficients est indiquée avec des étoiles. Ici nous avons **3 étoiles** (**`***`**), ce qui signifie que les coefficients sont très significatifs.

---

### Residuals 

Nous avons un **minimum à -0.12653** et un **maximum à 0.16835**. Ces valeurs montrent que les résidus restent dans un **intervalle restreint**. Cela signifie que le modèle fait des **erreurs relativement petites** et que ses **prédictions sont assez précises**.

De plus, **la médiane des résidus est proche de 0**, ce qui indique que le modèle est **équilibré** et que les erreurs sont réparties de manière homogène autour de la droite de régression. 

---

### Multiple R-squared

Le coefficient de détermination $R^2$ est de **0.9013**. Cela signifie que **90.13% de la variance de la variable dépendante $Y$ est expliquée par la variable explicative $X$**. Ce coefficient est très élevé, ce qui indique que le modèle linéaire est **très performant** pour prédire la quantité de bactéries en fonction du dosage de sérum.

## __10.__

On sait que : 

$$
SCR = \sum_{i=1}^n(\hat{y}- \bar{y})^2
$$ 

A l'aide de R, on trouve :

```{r echo=FALSE}

# Calcul de la somme des carrés des résidus
deviance(LMS)
sum(residuals(LMS)^2)
```
<br>

$$
SCR = 0.3311143
$$

---

### Interprétation

Plus la $SCR$ est faible, plus le modèle est performant. Dans notre cas, la $SCR$ est de **0.3311143**, ce qui indique que le modèle linéaire explique une grande partie de la variance de la variable dépendante $Y$. Cela confirme la pertinence du modèle pour prédire la quantité de bactéries en fonction du dosage de sérum.


## __11.__

<br>

### a\) Évaluer l'indépendance des résidus

<br>

#### ii\) 

<br>

##### **Qu'est ce que la fonction `acf()`** 

De manière générale, a fonction **`acf()`** permet de tracer la fonction d'autocorrélation d'une série temporelle. Elle permet de visualiser les corrélations entre les observations d'une série en fonction du décalage entre elles.

Voici le graphique obtenu à l'aide de R :

```{r echo=FALSE}

# Autocorrélation des résidus
residuals <- residuals(LMS)
acf(residuals, main = "Autocorrélation des résidus")
```

---

##### **Interprétation du graphique**

On voit qu'au début, les résidus sont fortement corrélés, puis la corrélation diminue progressivement pour devenir proche de zéro. Cela indique que les résidus ne sont pas indépendants les uns des autres et qu'il y a une certaine autocorrélation positive.

Ce résultat es assez logique. En effet, lors de la première administration du sérum, la quantité de bactéries diminue. Lors de la deuxième administration, la quantité de bactéries diminue à nouveau, mais elle est déjà plus faible qu'après la première administration. Ainsi, les résidus sont corrélés, car la quantité de bactéries dépend de la quantité précédente. 

On notera aussi le fait, que nous ne connaissons rien de cette bactérie, il est donc normal que la partie résiduelle soit élevé au début et diminue au fur et à mesure des administrations. Car l'individu se débarrasse de la bactérie et donc se rapproche d'un état de santé que nous pouvons expliqué. 

<br>

#### ii\)

<br>

Le test de Durbin-Watson (DW) est un test statistique utilisé en régression linéaire pour détecter l’autocorrélation des erreurs (résidus) dans un modèle. Autrement dit, il permet de vérifier si les résidus sont indépendants les uns des autres.

A l'aide de R, on obtient :

```{r echo=FALSE}

# Test de Durbin-Watson sur les résidus
dw_test <- dwtest(LMS)
print(dw_test)
```

Avant de commencer, dans ce test, il est fait l'hypothèse suivante : 

- $H_0$ : Les erreurs (résidus) ne sont pas autocorrélées. Autrement dit, elles sont indépendantes les unes des autres

---

##### **Comment interpréter les résultats ?**

Statistique de Durbin-Watson (DW) :

- Si **`DW`** est proche de **2**, cela signifie qu'il n'y a pas d'autocorrélation. 
- Si **`DW`** proche de **0**, cela indique une forte autocorrélation positive des résidus.
- Si **`DW`** proche de **4**, cela indique une forte autocorrélation négative des résidus.

---

Nous trouvons un **`DW` de 0.61628**. Cette valeur est **inférieure à 2**, ce qui suggère une **forte autocorrélation positive des résidus**.

De plus, la **`p-value`** associée à ce test est de **1.655e-09**, ce qui est **inférieur à 0.05**. Nous **rejetons donc l'hypothèse nulle** $H_0$ et concluons que les résidus ne sont pas indépendants les uns des autres.

Cette fois-ci, nous avons en plus **`alternative hypothesis`** qui est une hypothèse alternative. Celle-ci est **`positive`**, ce qui signifie que les résidus sont **positivement autocorrélés**.

---

##### **Conclusion général :**

Les résidus du modèle de régression linéaire ne sont pas indépendants les uns des autres. Il existe une forte autocorrélation positive entre les résidus.

<br>

### b\) Tester la normalité des résidus

<br>

#### i\) 

<br>

Pour rappel, nous faisons l'hypothèse suivante :

- **$H_0$ (Hypothèse nulle) :** Les données suivent une distribution normale.

A l'aide de R, on obtient :
```{r echo=FALSE}

# Test de Shapiro-Wilk sur les résidus
shapiro.test(residuals(LMS))
```

---

Le test appliqué aux résidus donne une statistique **`W` = 0.95739** avec une **p-valeur de 0.06909.**

Puisque **la p-valeur est supérieure à 0.05**, nous **ne rejetons pas l’hypothèse** de normalité.

Cela signifie que **les résidus `(SCR)` suivent une distribution normale** au seuil de 5%.

<br>

#### i\)

```{r echo=FALSE, fig.align='center'}

# Diagramme Quantile-Quantile (Q-Q plot) des résidus :

qqnorm(residuals, main = "Q-Q Plot des résidus")
qqline(residuals, col = "red")
```

---

##### **Interprétation du Q-Q Plot**

- **Alignement des points** :  
  Les points du Q-Q plot des **résidus** sont quasiment alignés sur la droite diagonale (celle tracée en rouge). Cela suggère que les quantiles observés des **résidus** correspondent étroitement aux quantiles théoriques d’une distribution normale.

- **Extrémités (queues de distribution) :**  
  Les points situés aux extrémités ne s’écartent pas significativement de la droite, ce qui renforce l’idée que les **résidus** présente les caractéristiques d’une distribution normale.

Les **résidus semblent donc être normalement distribué.**

---

##### **Conclusion générale**

<br>

Le test de **Shapiro-Wilk** et le **Q-Q plot** des résidus indiquent que ces derniers suivent une **distribution normale**. Cela confirme que les résidus du modèle de régression linéaire sont bien distribués selon une **loi normale**.

## __12.__

### **Un peu de mathématiques !** 

Nous cherchons à ajuster un modèle exponentiel de la forme :

$$
y = b \cdot a^x
$$

où $a$ et $b$ sont les paramètres à estimer à partir des données $(X, Y)$.

### **Transformation logarithmique**
L’équation exponentielle n’étant pas linéaire, nous appliquons le logarithme népérien des deux côtés :

$$
\ln y = \ln b + x \ln a
$$

En posant :

$$
Y = \ln y, \quad A = \ln a, \quad B = \ln b,
$$

nous obtenons une équation linéaire :

$$
Y = A X + B
$$

::: {.alert .alert-warning}
**Attention :** Ici, $Y$ représente les valeurs transformées $\ln y$, et **non** les valeurs originales de $Y$.
:::

Nous pouvons maintenant estimer les paramètres $A$ et $B$ à l'aide d'une régression linéaire entre $X$ et $Y$.

### **Estimation des paramètres**
La pente de la droite de régression est donnée par :

$$
A = \frac{\text{Cov}(X, Y)}{\sigma^2_X}
$$

où $\text{Cov}(X, Y)$ est la covariance entre $X$ et $Y$, et $\sigma^2_X$ est la variance de $X$.

::: {.alert .alert-warning}
**Attention :** Ce coefficient $A$ correspond à $\ln a$, **et non** directement à $a$.
:::

L'ordonnée à l'origine est :

$$
B = \overline{Y} - A \cdot \overline{X}
$$

où $\overline{Y}$ et $\overline{X}$ sont respectivement les moyennes de $Y$ et $X$.

### **Récupération des coefficients $a$ et $b$**

Étant donné que $A = \ln a$ et $B = \ln b$, nous retrouvons les valeurs de $a$ et $b$ par exponentiation :

$$
 a = e^A, \quad b = e^B.
$$

### **Modèle final**

En réinjectant ces valeurs dans l'équation exponentielle initiale, nous obtenons la fonction qui traduit la courbe ajustée :

$$
 y = e^B \cdot (e^A)^x
$$

Par définition des exponentielles, cela revient à :

$$
 y = e^B \cdot e^{Ax}
$$


### **Conclusion**

Grâce à cette méthode, nous avons pu estimer les paramètres $a$ et $b$ du modèle exponentiel en appliquant une régression linéaire sur les valeurs transformées $\ln Y$. L'exponentiation des coefficients obtenus nous permet de retrouver les valeurs finales de $a$ et $b$, garantissant un ajustement correct du modèle exponentiel aux données.

---

```{r echo=FALSE}

# Transformation logarithmique des données
y_log <- log(data$Y)

# Covariance du nouveau modèle
covariance_log <- sum(data$X * y_log) / length(data$X) - mean(data$X) * mean(y_log)

# Calcul des paramètres A et B
a_log <- covariance_log / var_X_pearson
b_log <- exp(mean(y_log) - a_log * mean(data$X))


cat("A =", a_log, "B =", b_log)
```

<br>

### **Modèle exponentiel ajusté**

$$
y = 1.034294 \times e^{-0.2093764 x}
$$
<br>

```{r echo=FALSE, fig.align='center'}

# Fonction exponentielle ajustée
exponential_function <- function(x) {
  b_log * exp(a_log*x)
}

# Nuage de points avec droite de régression exponentielle + linéaire
ggplot(data, aes(x = X, y = Y)) +
  geom_point(color = "red", size = 2) +
  geom_abline(slope = -0.0835, intercept = 0.8565, color = "blue", linewidth = 1) +
  stat_function(fun = exponential_function, color = "green", linewidth = 1) +
  labs(
    title = "Nuage de points : Dosage vs Bactéries",
    x = "Dosage de sérum (mL)",
    y = "Quantité de bactéries (UFC/mL)"
  ) +
  theme_fivethirtyeight() +
  theme(axis.title = element_text(), text = element_text(family = "Segoe UI", size = 14))
```


## __13.__


L’objectif est de comparer les modèles **linéaire** et **exponentiel** en utilisant le coefficient de détermination $R^2$.

Le coefficient de détermination $R^2$ est défini comme :

$$
R^2 = \frac{SCE}{SCT}
$$

où :

- $SCE$ est la Somme des Carrés Expliquée par le modèle.
- $SCT$ est la Somme des Carrés Totale, représentant la variabilité totale des données.


### **Calcul de $R^2$ pour les modèles linéaire et exponentiel**

<br>

#### **Modèle linéaire**

En utilisant R :

```{r echo=FALSE}

# Calcul de R² pour le modèle linéaire
summary(LMS)$r.squared
```

Le résultat obtenu est :

$$
R^2_{lin} = 0.9013
$$

---

#### **Modèle exponentiel**

A l'aide du logarithme népérien, nous avons estimé les paramètres $A$ et $B$ du modèle exponentiel. En utilisant R, on trouve le R² suivant :


```{r echo=FALSE}
# Ajustement du modèle exponentiel
exponential_model <- lm(log(Y) ~ X, data = data)

# Calcul de R² pour le modèle exponentiel
summary(exponential_model)$r.squared
```

Le résultat obtenu est donc :

$$
R^2_{exp} = 0.9151
$$

---

#### **Interprétation des résultats :**

Les valeurs obtenues sont :

- **Modèle linéaire** : $R^2 = 0.9013$
- **Modèle exponentiel** : $R^2 = 0.9151$

On observe que le modèle exponentiel présente un **coefficient $R^2$ légèrement plus élevé** que le modèle linéaire. Cela signifie que l’ajustement exponentiel explique **un peu mieux** la variabilité des données.

---

### Interprétation des résultats avec `summary()`
Outre $R^2$, les résultats de `summary()` nous donnent plusieurs indications sur les modèles :

### 1. **Analyse du modèle exponentiel**

- **P-value du coefficient** : $< 2.2e^{-16}$ (très significatif)
- **Erreur standard résiduelle** : $0.1917$
- **F-statistic** : $517.5$ (indique une forte relation entre $X$ et $\log(Y)$)

#### **Interprétation :**

- Le coefficient de $X$ est **très significatif**, ce qui prouve que $X$ a un impact clair sur $\log(Y)$.
- L’erreur standard est relativement faible, ce qui montre une bonne précision du modèle.

<br>

### 2. **Analyse du modèle linéaire**

- **P-value du coefficient** : $< 2.2e^{-16}$ (très significatif)
- **Erreur standard résiduelle** : $0.08306$ (plus faible que l’exponentiel)
- **F-statistic** : $438.5$

#### **Interprétation :**

- Le modèle linéaire est également **très significatif**.
- Son erreur standard résiduelle est **plus faible** que le modèle exponentiel, ce qui peut indiquer une meilleure précision locale.

<br>

### Comparaison des modèles

| Critère                     | Modèle exponentiel | Modèle linéaire |
|:--                          |:--                 |:--              |
| **$R^2$**                   | **0.9151**         | 0.9013          |
| **Erreur résiduelle**       | 0.1917             | **0.08306**     |
| **Pente ($a$ ou $\ln(a)$)** |-0.209       4      | -0.0835         |
| **P-value**                 |< 2.2e-16           |< 2.2e-16        |

--- 

### **Analyse finale :**

Le **modèle exponentiel** a un $R^2$ **légèrement plus élevé**, suggérant qu'il explique mieux la variance des données.

Le **modèle linéaire a une erreur résiduelle plus faible**, ce qui peut indiquer une meilleure précision locale.

**Les deux modèles sont hautement significatifs (p-value < 2.2e-16)**, donc l’un comme l’autre sont pertinents.


## __14.__

Nous cherchons à calculer, à partir de chacun des deux modèles, une prévision de la quantité de bactérie encore présente dans le corps 24 heures après l’injection d’une dose de 8 mL de sérum.

### **Modèle exponentiel**

Dans le modèle exponentiel, nous avons :

$$
y = 1.034294 \times e^{-0.2093764 x}
$$

En remplaçant $x = 8$ :

```{r}

cat("Y exponentiel =", 1.034294 * exp(-0.2093764 * 8))
```

### **Modèle linéaire**

Dans le modèle linéaire, nous avons :

$$
\hat{y} = -0,083513x + 0,856489
$$

En remplaçant $x = 8$ :

```{r}

cat("Y linéaire =", -0.083513 * 8 + 0.856489)
```

---

### **Résultats**

- **Modèle exponentiel** : $0.1943$
- **Modèle linéaire** : $0.1885$

---

### **Interprétation**

Les deux modèles donnent des prévisions différentes pour la quantité de bactéries restante 24 heures après l’injection d’une dose de 8 mL de sérum. 

## __Bonus__


On cherche dans le modèle exponentiel à résoudre l'équation suivante :
 
$$
y = 0 \Longleftrightarrow 1.034294 \times e^{-0.2093764 x} = 0 
$$

On cherche dans le modèle linéaire à résoudre l'équation suivante :

$$
\hat{y} = 0 \Longleftrightarrow -0,083513x + 0,856489 = 0 
$$
$$
\Longleftrightarrow -0,083513x = -0,856489
$$

$$
\Longleftrightarrow x = \frac{-0,856489}{-0,083513}
$$

```{r}

cat("X linéaire =", ((-0.856489)/(-0.083513))*24)

```





