---
title: "Statistiques Descriptives 2"
author: "Ibrahim BENKHERFELLAH & Axel COULET"
date: "2025-02-21"
output:
  html_document:
    df_print: paged
lang: fr
---

```{r eval=FALSE, include=FALSE}
# Run this the first time you open the document to install all the required packages

list.of.packages <- c("ggplot2", "ggthemes", "extrafont", "gridExtra", "lmtest")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[, "Package"])]
if (length(new.packages)) install.packages(new.packages)
```

```{r echo=FALSE, message=FALSE}
# Run these at the start

library(ggplot2)
library(ggthemes)
library(extrafont)
library(gridExtra)
library(lmtest)
```

```{r eval=FALSE, include=FALSE}
# Run these only one time
font_import()
loadfonts()
```


# __Exercice n°1__

## __1.__\

<br>
Tout d'abord, qu'est-ce qu'une variable explicative et une variable expliquée ?

-   Une variable explicative est la variable qui permet d'expliquer une autre variable. Elle est souvent notée $X$. Sur une représentation graphique, la variable explicative est représentée en abscisse.

-   Une variable expliquée est la variable pour laquelle on cherche un modèle de prévision. Son évolution dépend d'une ou de plusieurs autres variables. Elle est souvent notée $Y$. Sur une représentation graphique, la variable expliquée est représentée en ordonnée.\

Ainsi, dans le cadre de notre exercice, la variable explicative est le **dosage de sérum X (en mL)**. Et la variable expliquée est la **quantité de bactéries Y (en UFC/mL)**, car c'est celle qui est mesurée en fonction du dosage de sérum et qui dépend potentiellement de $X$.\
<br>

## __2.__\

Représentation du nuage de points de la statistique bi-variée ($x_{i};y_{i}$) :

<br>
```{r echo=FALSE, fig.align='center'}

data <- read.csv("../donnees_r.csv", header = TRUE,  sep = ",", colClasses = c("numeric", "numeric"))

ggplot(data, aes(x = X, y = Y)) +
  geom_point(color = "red", size = 2) +
  labs(
    title = "Nuage de points : Dosage vs Bactéries",
    x = "Dosage de sérum (mL)",
    y = "Quantité de bactéries (UFC/mL)"
  ) +
  theme_fivethirtyeight() +
  theme(axis.title = element_text(), text = element_text(family = "Segoe UI", size = 14))

```
<br>

## __3.__\

### a\) Calcul de la moyenne et de la variance de la quantité de bactéries

<br>

- Moyenne : __$\overline{y} = 0,4389205$__

- Variance : __$\sigma^2 = 0,0671159$__

<br>

### b\) Box-plot de la dsitribution de la variable : __Quantité de bactéries__

<br>
```{r echo=FALSE, fig.align='center'}
ggplot(data, aes(y = Y)) +
  geom_boxplot() +
  labs(title = "Boîte à moustache de la distribution\nde la quantité de bactéries") +
  theme_fivethirtyeight() +
  theme(text = element_text(family = "Segoe UI", size = 14))
```

<br>

### c\) Analyse des résultats obtenus

L'analyse de la quantité de bactéries montre une moyenne de __$0,4389205$__ et une variance de __$0,0671159$__, indiquant une dispersion modérée. Le diagramme en boîte révèle une distribution symétrique avec une médiane autour de __$0,4$__ et un intervalle interquartile de __$0,23$__ à __$0,60$__, suggérant une répartition homogène des données.

## __4.__

Nous savons que la Covariance de $X$ et $Y$ est : $$Cov(X,Y) = \frac{1}{N}\sum_{i=1}^{i=N}(X_i - \overline{X})(Y_i - \overline{Y}) = (\frac{1}{N}\sum_{i=1}^{i=N}(X_iY_i)) - \overline{X}\overline{Y}$$\

On trouve donc : __$Cov(X, Y) = -0,7243548$__\

La covariance entre les deux variables étant de __$-0,7243548$__, cela indique une assez forte relation. Le signe étant négative, cela  indique que $X$ et $Y$ évoluent de manière opposé. Autrement dit, lorsque $X$ augmente, $Y$ quant à lui à diminuer, et inversement. Ce qui coincide avec l'observation faite ci-dessus, dans le nuage de point.

## __5.__

Les Q-Q plots (Quantile-Quantile plots) permettent de comparer les quantiles d'une variable observée à ceux d'une distribution théorique, souvent la distribution normale. En examinant l'alignement des points sur une droite diagonale, il est possible d'évaluer visuellement si une variable suit une distribution normale.

### a\) Etude graphique Quantile-Quantile

<br>
```{r echo=FALSE, fig.align='center'}
qqX <- ggplot(data) +
  geom_qq(aes(sample = X), geom = "point", position = "identity") +
  ggtitle("Q-Q Plot de X") +
  theme_fivethirtyeight() +
  theme(text = element_text(family = "Segoe UI", size = 14))

qqY <- ggplot(data) +
  geom_qq(aes(sample = Y), geom = "point", position = "identity") +
  ggtitle("Q-Q Plot de Y") +
  theme_fivethirtyeight() +
  theme(text = element_text(family = "Segoe UI", size = 14))

grid.arrange(qqX, qqY, ncol = 2)
```
<br>

#### Interprétation du Q-Q Plot pour la variable `X`

- **Alignement des points** :  
  Les points du Q-Q plot de **$X$** sont quasiment alignés sur la droite diagonale. Cela suggère que les quantiles observés de **$X$** correspondent étroitement aux quantiles théoriques d'une distribution normale.

- **Extrémités (queues de distribution)** :  
  Les points situés aux extrémités ne s'écartent pas significativement de la droite, ce qui renforce l'idée que **$X$** présente les caractéristiques d'une distribution normale.

La variable **$X$ semble être normalement distribuée.**

---

#### Interprétation du Q-Q Plot pour la variable `Y`

- **Écart par rapport à la diagonale** :  
  Pour la variable **$Y$**, les points du Q-Q plot ne suivent pas la droite diagonale. On observe une courbure, surtout aux extrémités, indiquant que les quantiles observés ne correspondent pas aux quantiles attendus pour une distribution normale.

- **Queues de distribution** :  
  Les écarts importants aux extrémités suggèrent que **$Y$** possède des caractéristiques non conformes à la normalité, telles qu'une asymétrie ou des queues de distribution différentes de celles d'une loi normale.

La variable **$Y$ ne semble pas suivre une distribution normale**.


### b\) Etude numérique Shapiro-Wilk

<br>
Le **test de Shapiro-Wilk** est un test statistique permettant d’évaluer si un échantillon suit une **distribution normale**. Il est particulièrement efficace pour les **petites tailles d’échantillon** ($n \leq 50$), bien qu'il puisse être utilisé sur des ensembles de données plus grands.

---

#### Hypothèses du test

<br>

- **$H_0$ (Hypothèse nulle) :** Les données suivent une distribution normale.
- **$H_1$ (Hypothèse alternative) :** Les données ne suivent pas une distribution normale.

---

#### Formule du test

La statistique du test de Shapiro-Wilk est définie comme suit :

$$W = \frac{( \sum_{i=1}^{n} a_i x_{(i)} )^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} $$

**où** :

- $x_{(i)}$ sont les valeurs de l’échantillon classées par ordre croissant.
- $\bar{x}$ est la **moyenne** de l’échantillon.
- $a_i$ sont des coefficients calculés à partir des quantiles d'une distribution normale.

---

#### Interprétation des résultats

- Si la **p-valeur** du test est **supérieure à 0.05**, on **ne rejette pas** $H_0$ : l’échantillon peut être considéré comme **normalement distribué**.
- Si la **p-valeur** est **inférieure à 0.05**, on **rejette** $H_0$ : les données **ne suivent pas** une distribution normale.

---

#### Résultats du code R

```{r echo=FALSE}
shapiro.test(data$X)
shapiro.test(data$Y)
```

---

#### Variable `X`  

Le test appliqué à **`X`** donne une statistique **W = 0.95558** avec une **p-valeur de 0.05809**.

Puisque la **p-valeur est supérieure à 0.05**, nous **ne rejetons pas** l'hypothèse de normalité.

Cela signifie que **la variable `X` suit une distribution normale** au seuil de 5%.  

---

#### Variable `Y`  

Le test appliqué à **`Y`** donne une statistique **W = 0.92148** avec une **p-valeur de 0.002668**.  

Dans ce cas, la **p-valeur est inférieure à 0.05**, nous **rejetons donc** l'hypothèse de normalité. 

Ainsi, nous pouvons conclure que **la variable `Y` ne suit pas une distribution normale**.  


## __6.__

On sait que le coefficient de corrélation linéaire de Pearson se calcul de la façon suivante :

$$ r(X,Y) = Corr(X,Y) = \frac{Cov(X,Y)}{\sigma_X^2\sigma_Y^2}$$
Avec R, on trouve : 

```{r echo=FALSE}
cor.test(data$X, data$Y, method = "pearson")
```

---

Le coefficient de corrélation linéaire de Pearson est de __$-0,9493844$__. Ce coefficient est très proche de -1, ce qui indique une forte corrélation négative entre les deux variables. Cela signifie que $X$ et $Y$ évoluent de manière opposée (comme supposé avec la covariance). Cette relation est donc confirmée par le coefficient de corrélation élevé, qui suggère une forte dépendance linéaire entre les deux variables. 
<br>

## __7.__

On sait qu'une droite affine s'exprime de la façon suivante : **$y = ax + b$**

Par la méthode de Legendre, aussi appelé méthode des moindres carrés, on peut trouver les coefficients $a$ et $b$ de la droite de régression linéaire qui minimise la somme des carrés des résidus. Autrement dit, on cherche $\hat{a}$ et $\hat{b}$ tels que :

$$f(\hat{a},\hat{b}) = \arg\min_{(a,b)}\sum_{i=1}^N(y_i - (ax + b))^2$$ 
Cela signifie que nous cherchons les valeurs spécifiques de $a$ et $b$ qui rendent la somme des carrés des erreurs aussi petite que possible. D'après le cours de monsieur Hébert.


Dans un modèle linéaire, le meilleur coefficient directeur (au sens des moindres carrés) et le meilleur ordonné à l'origine (au sens des moindres carrés) sont :

$$\hat{a} = \frac{cov(X, Y)}{\sigma^2}$$
**et**

$$\bar{b} = \bar{y} - \hat{a}x $$ 

### **Démonstration** 

$$\frac{\partial f}{\partial a} = \sum_{i=1}^N (2*(y_i - ax_i -n)(-x_i))$$
<br>

$$\frac{\partial f}{\partial b} = \sum_{i=1}^N (2*(y_i - ax_i -n)(-1))$$
<br>

**On cherche :** $~~ \frac{\partial f}{\partial a} = 0~~$ et  $~~ \frac{\partial f}{\partial b} = 0~~$

<br>

D’où le système suivant :

$\begin{cases}
\sum 2(y_i - ax_i - n)(-x_i) = 0\\
\sum 2(y_i - ax_i - n)(-1) = 0
\end{cases}
~ ~ 
\xrightarrow[L_2 \leftarrow L_2 \cdot \frac{1}{2}]{L_1 \leftarrow L_1 \cdot \frac{1}{2}}
~ ~
\begin{cases}
\sum (y_i - ax_i - n)(-x_i) = 0\\
\sum (y_i - ax_i - n)(-1) = 0
\end{cases}
~ ~
\xrightarrow[L_2 \leftarrow L_2 \cdot \frac{1}{N}]{L_1 \leftarrow L_1 \cdot \frac{1}{N}}
~ ~
\begin{cases}
\overline{(y - ax - b)x} = 0\\
\overline{y - ax - b} = 0
\end{cases}
~ ~
\longrightarrow
~ ~
\begin{cases}
\overline{xy} - a\bar{x^2} - b\bar{x} = 0\\
\bar{y} - a\bar{x} - b = 0
\end{cases}
~ ~
\xrightarrow{L_2 \leftarrow L_2 \cdot \bar{x}}
~ ~
\begin{cases}
\overline{xy} - a\bar{x^2} - b\bar{x} = 0\\
\bar{x} \bar{y} - a\bar{x}^2 - b\bar{x} = 0
\end{cases}
~ ~
\xrightarrow{L_1 - L_2}
~ ~
(\overline{xy} - \bar{x} \bar{y}) - a(\bar{x^2} - \bar{x}^2) = 0
~ ~
\longrightarrow
~ ~ 
Cov(X,Y) - a\sigma_X^2 = 0$

<bar>

**D'ou** :

$\hat{a} = \frac{cov(X, Y)}{\sigma^2} ~~~$ et $~~~\bar{b} = \bar{y} - \hat{a}x$ 

---

### Applications 

<br> 

$$\hat{a} = \frac{-0,7243548}{8,673469} = -0,083513$$

**et**

$$\hat{b} = 0,4389205 - (-0,083513*5) = 0,856489$$ 

--- 

### Récapitulatif

<br>
$$\begin{cases}
\hat{a} = 0,083513\\
\hat{b} = 0,856489
\end{cases}
~ ~ \Longrightarrow  ~ ~ ~ \hat{y} ~ = ~ -0,083513x ~ + ~ 0,856489$$

<br>
```{r echo=FALSE, fig.align='center'}
ggplot(data, aes(x = X, y = Y)) +
  geom_point(color = "red", size = 2) +
  geom_abline(slope = -0.083513, intercept = 0.856489, color = "blue", linewidth = 1) +
  labs(
    title = "Nuage de points : Dosage vs Bactéries",
    x = "Dosage de sérum (mL)",
    y = "Quantité de bactéries (UFC/mL)"
  ) +
  theme_fivethirtyeight() +
  theme(axis.title = element_text(), text = element_text(family = "Segoe UI", size = 14))
```
<br>

## __8.__

En observant le nuage de points, on voit clairement que lorsque le dosage de sérum augmente, la quantité de bactéries diminue de manière régulière. La droite de régression, qui semble passer naturellement au milieu de ces données, reflète bien cette tendance. Cette modélisation linéaire nous paraît être une première approximation logique et cohérente, en accord avec l'idée que l'augmentation du dosage se traduit par une diminution de la concentration bactérienne. L'ajustement linéaire offre ici une représentation intuitive et satisfaisante de la relation observée.

## __9.__

```{r echo=FALSE}
LMS <- lm(data$Y ~ data$X)
summary(LMS)

#plot(summary(LMS)$residuals, main = "Distribution des résidus", ylab = "Résidus", xlab = "Observations")
#abline(h = 0, col = "red", lwd = 2)

```

---

### Test de significativité globale du modèle

Avant d'interpréter les résultats, il convient d'évaluer la significativité du modèle. Il désigne le seuil à partir duquel les résultats d'un test sont jugés fiables. Autrement dit, ce seuil détermine la confiance dans la corrélation entre un test effectué et les résultats obtenus.

Soit : $H_0$ -> le modèle n'est pas significatif 

Ce test est basé sur la **statistique de Fisher** présentée en bas de la sortie R. La p-valeur associée à cette statistique est de **`2e-16`**. Puisque cette valeur est **inférieure à 1%**, nous **rejetons l'hypothèse nulle**, à savoir le modèle est bien **globalement significatif**

---

### Interprétation des coefficients

Les coefficients permettent de comprendre comment la variable explicative $X$ influence la variable dépendante $Y$.

**`(Intercept)`** qui est dans notre cas $~\hat{b}~$ (que nous allons appelé $~\beta_0$), est de **0.85649**. Et **`data$X`** qui est $~\hat{a}~$ (que nous allons appelé $~\beta_1$), est de **-0.08351**. Ces valeurs correspondent à celles que nous avons calculées précédemment.

Les **`Pr(>|t|)`** sont les **`p-valeurs`** associées à chaque coefficient. Elles permettent de tester **l'hypothèse nulle** $H_0$ suivante : 

- $H_0 : \beta_i = 0 ~$ -> Cela signifie que $X$ n'a aucun effet sur $Y$

Or, ici les **p-valeurs sont très faibles** (**`2e-16`**), ce qui signifie que nous **rejetons l'hypothèse nulle**. Donc **les coefficients sont significatifs** et que la variable explicative $X$ a un effet significatif sur la variable dépendante $Y$. 

De plus, dans R la signification des coefficients est indiquée avec des étoiles. Ici nous avons **3 étoiles** (**`***`**), ce qui signifie que les coefficients sont très significatifs.

---

### Residuals 

Nous avons un **minimum à -0.12653** et un **maximum à 0.16835**. Ces valeurs montrent que les résidus restent dans un **intervalle restreint**. Cela signifie que le modèle fait des **erreurs relativement petites** et que ses **prédictions sont assez précises**.

De plus, **la médiane des résidus est proche de 0**, ce qui indique que le modèle est **équilibré** et que les erreurs sont réparties de manière homogène autour de la droite de régression. 

---

### Multiple R-squared

Le coefficient de détermination $R^2$ est de **0.9013**. Cela signifie que **90.13% de la variance de la variable dépendante $Y$ est expliquée par la variable explicative $X$**. Ce coefficient est très élevé, ce qui indique que le modèle linéaire est **très performant** pour prédire la quantité de bactéries en fonction du dosage de sérum.

## __10.__

```{r echo=FALSE}
deviance(LMS)
sum(residuals(LMS)^2)
```

## __11.__

a\) Évaluer l’indépendance des résidus

i) 

```{r echo=FALSE}
residuals <- residuals(LMS)
acf(residuals, main = "Autocorrélation des résidus")
```

i)

```{r echo=FALSE}
dw_test <- dwtest(LMS)
print(dw_test)
```

b\) Tester la normalité des résidus

i) 

Shapiro-Wilk

i)

Quantile-Quantile (Q-Q plot)

## __12.__

$\log{y} = \log{(b.ax)} = \log{b} + x.\log{a}$  
$\log{y} = \log{(b.ax)} = -0,067277834 + x.-1,078241507$


```{r echo=FALSE, fig.align='center'}
exponential_function <- function(x) {
  1.0343 * exp(-0.209 * x)
}

ggplot(data, aes(x = X, y = Y)) +
  geom_point(color = "red", size = 2) +
  geom_abline(slope = -0.0835, intercept = 0.8565, color = "blue", linewidth = 1) +
  stat_function(fun = exponential_function, color = "green", linewidth = 1) +
  labs(
    title = "Nuage de points : Dosage vs Bactéries",
    x = "Dosage de sérum (mL)",
    y = "Quantité de bactéries (UFC/mL)"
  ) +
  theme_fivethirtyeight() +
  theme(axis.title = element_text(), text = element_text(family = "Segoe UI", size = 14))
```

## __13.__

1. 

???? -> $R^2$

1.

????

## __14.__

$0,1885 = -0,0835\times8+0,8565$
$0,1943 = 1,0343e^{-0,209\times8}$

## __Bonus__

?????

## Valeur peut être utile

```{r echo=FALSE}
var(data$Y)
var(data$X)
mean(data$Y)
mean(data$X)
cov(data$X, data$Y, method = "pearson")
median(data$Y)
quantile(data$Y)
```
